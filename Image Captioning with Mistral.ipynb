{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers accelerate ctransformers langchain torch pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type, Optional\n",
    "import json\n",
    "from langchain.llms import CTransformers\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './model/mistral-7b-instruct-v0.1.Q5_0.gguf'  # Replace this with the path to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"max_new_tokens\": 2048,\n",
    "    \"context_length\": 4096,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"stream\": True,\n",
    "    \"threads\": int(os.cpu_count() / 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model=MODEL_PATH,\n",
    "                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01216560",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886feb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c737f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "class ImageCaptionerInput(BaseModel):\n",
    "    image_url: str = Field(description=\"URL of the image that is to be described\")\n",
    "\n",
    "\n",
    "@tool(\"image_captioner\", return_direct=True, args_schema=ImageCaptionerInput)\n",
    "def image_captioner(image_url: str) -> str:\n",
    "    \"\"\"Provides information about the image\"\"\"\n",
    "    raw_image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
    "    inputs = blip_processor(raw_image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs)\n",
    "    return blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "tools = [image_captioner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63112a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n",
    "\n",
    "class OutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        try:\n",
    "            # this will work IF the text is a valid JSON with action and action_input\n",
    "            response = parse_json_markdown(text)\n",
    "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                # this means the agent is finished so we call AgentFinish\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                # otherwise the agent wants to use an action, so we call AgentAction\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            # sometimes the agent will return a string that is not a valid JSON\n",
    "            # often this happens when the agent is finished\n",
    "            # so we just return the text as the output\n",
    "            return AgentFinish({\"output\": text}, text)\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"conversational_chat\"\n",
    "\n",
    "\n",
    "# initialize output parser for agent\n",
    "parser = OutputParser()\n",
    "\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=memory,\n",
    "    agent_kwargs={\"output_parser\": parser}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa031f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_instruct = \"<s>[INST] \"\n",
    "end_instruct = \"[/INST]\"\n",
    "end_sentence = \"</s>\"\n",
    "system_message_plain = \"\"\"Assistant is a expert JSON builder designed to help user describe images.\n",
    "\n",
    "Assistant is able to respond to the User and use tools using JSON strings that contain \"action\" and \"action_input\" parameters.\n",
    "\n",
    "All of Assistant's communication is performed using this JSON format.\n",
    "\n",
    "Assistant can also use tools by responding to the user with tool use instructions in the same \"action\" and \"action_input\" JSON format. Tools available to Assistant are:\n",
    "\n",
    "- \"image_captioner\": Useful when you need to get information about the image\n",
    "  - To use the image_captioner tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"action\": \"image_captioner\",\n",
    "      \"action_input\": \"https://xyz.png\"}}\n",
    "    ```\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\"\"\"\n",
    "system_message = start_instruct + system_message_plain + end_instruct + end_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": 'Hey how are you today?'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"I'm good thanks, how are you?\"}}\n",
    "```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I'm great, what is this image about - https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg ?\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"image_captioner\",\n",
    " \"action_input\": \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"}}\n",
    "```'''\n",
    "        },{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"a woman sitting on the beach with her dog\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"This image shows a woman sitting on the beach with her dog\"}}\n",
    "```'''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Thanks could you now tell me what is in this image: https://www.adorama.com/alc/wp-content/uploads/2015/05/stories-HRX5WXFyB64-unsplash.jpg\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"image_captioner\",\n",
    " \"action_input\": \"https://www.adorama.com/alc/wp-content/uploads/2015/05/stories-HRX5WXFyB64-unsplash.jpg\"}}\n",
    "```'''\n",
    "        },{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"a beach with sun setting in the background\"\n",
    "        },\n",
    "            {\"role\": \"assistant\",\n",
    "            \"content\": '''```json\n",
    "{{\"action\": \"Final Answer\",\n",
    " \"action_input\": \"The image is of a sunset on the beach\"}}\n",
    "```'''\n",
    "        }\n",
    "]\n",
    "\n",
    "conversation_formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e09b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = system_message + '\\n\\n' + conversation_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c22ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = agent.agent.create_prompt(\n",
    "    system_message=prompt,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b17aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt = new_prompt\n",
    "\n",
    "instruction = start_instruct + \" Respond to the following in JSON with 'action' and 'action_input' values \" + end_instruct\n",
    "human_msg = instruction + \"\\nUser: {input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebebbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = agent(\"Explain this image: https://images.hindustantimes.com/auto/img/2023/07/23/1600x900/Tesla_Cybertruck_1688887534001_1690087911053.jpeg\")\n",
    "resp['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c843a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = agent('Where was the Tesla car parked?')\n",
    "resp['output']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
